\documentclass[11pt,a4page]{article}

\usepackage{fullpage}
\usepackage{url}
\usepackage{helvet}
\usepackage{bold-extra}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,amsmath}

\title{\textbf{\fontfamily{\sfdefault}\selectfont DynDTA: Proposal for a Dynamic Data Transfer Agent for the CMS Experiment}}
\author{Bj\"{o}rn Barrefors\\
  University of Nebraska-Lincoln\\
  \texttt{bbarrefo@cse.unl.edu}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

	The CMS experiment is a world-wide physics collaboration storing around O(50PB) of data on more than 50 sites across 4 continents. Managing all this data is a manual job taking time from both phsyicist and data managers at each site. New data is expected to be generated by the end of 2014 further increasing the task. This proposal suggests and investigates an automated and intelligent data agent for data replication and deletion. Two equations are adopted to measure system-wide balance to respectively measure the effectiveness of storage and CPU utilization. We develop a daily system-wide storage and load balancing algorithm and provide measurements to prove its effectiveness.

\end{abstract}


\section{\textsc{Introduction}}

  An increase in the amount of data generated by scientific experiments [cite] combined with an increased awareness of energy consumption [cite] the last years have increased the demand for more efficient data placement in large computing grids. By the end of 2014 a new set of experiments will start up at the LHC [cite]. All data is not equally popular [cite], there are datasets analyzed by several physics groups while other datasets are only of interest to a few scientists. Currently when a dataset is needed the group or scientist demanding it will request a replica to his or her site [cite]. But a new computing model is emerging where instead of transferring data to the jobs, jobs are transferred to the data to decrease bandwidth usage and data utilization [cite]. In this model a job can be executed almost instantly as there is no need to transfer up to almost 100TB of data. The model is not perfect, in case of a popular dataset the site(s) hosting replicas can become overflown with jobs causing a CPU usage delay instead of transfer delay. A new data management policy is needed for popularity based dataset replication for optimal data and CPU availability.
  In this paper we investigate possible measurements of popularity and suggest a popularity based system-wide storage and load balancing algorithm. A series of metrics are suggested for measuring success in terms of storage and load balancing combined with costs of keeping a balanced system.


\section{\textsc{Related Work}}

  


\section{\textsc{Data management algorithm}}

  \subsection{Dataset Popularity}
    Creating a balanced system is of importance as it will both decrease the amount of data which just takes up space without being accessed and decrease the number of accesses on particular data which could cause overload on either network or CPU's. We have come up with a balance equation which can be continiously optimized to keep the system balanced. Using the standard deviation of all datasets ratio between number of accesses and number of replicas we can get a good measurement of the system balance. Combining equation \ref{eq:delta_t} and \ref{eq:std_dev} achieves this. Notice that the size of each dataset is added in to balance the equation based on dataset size. Here $n\_access$ is the number of accesses during a time period $t$, $size\_GB$ is the size of the dataset, and $n\_replicas$ is the number of replicas for the dataset. $\mathcal{N}$ is the set of all datasets.

    \begin{equation}\label{eq:delta_t}
      \delta = \int_0^t \! \frac{n\_accesses}{size\_GB*n\_replicas} \, \mathrm{d}t
    \end{equation}

    \begin{equation}\label{eq:std_dev}
      \sigma = \sqrt{\frac{\sum\limits_{d \in \mathcal{D}} (\delta_d - \overline{\delta})^2}{|\mathcal{D}| - 1}}
    \end{equation}

    The goal is to minimize equation \ref{eq:std_dev}, using a variation of frequency vectors we propose to use algorithm \ref{alg:data_sel}. Because the algorithm need to differentiate between increasingly popular and decreasingly popular datasets we introduce $\delta_f$ as the change in frequency in equation \ref{eq:delta_f}, $\delta_f < 0$ means the dataset have more then average replicas per GB and $\delta_f > 0$ means the dataset have less then average replicas per GB. The closer to $0$ the better. The algorithm uses exponential weight to avoid temporary spikes or drops in usage to trigger a subscription or deletion. We use a time window of $w$ days, each day with a $\delta_f$, where days further away are given an inverse exponential weight. We also define $\delta_{f+}$, $\delta_{f-}$, and $available\_space$ in equations \ref{eq:delta_fp}, \ref{eq:delta_fm}, and \ref{eq:avail}.

    \begin{equation}\label{eq:delta_f}
      \delta_f = \delta - \overline{\delta}
    \end{equation}

    \begin{equation}\label{eq:delta_fp}
      \delta_{f+} = \int_0^t \! \frac{n\_accesses}{size\_GB*(n\_replicas+1)} \, \mathrm{d}t
    \end{equation}

    \begin{equation}\label{eq:delta_fm}
      \delta_{f-} = \int_0^t \! \frac{n\_accesses}{size\_GB*(n\_replicas-1)} \, \mathrm{d}t
    \end{equation}

    \begin{equation}\label{eq:avail}
      available\_space = total\_space*0.95 - used\_space
    \end{equation}

    \begin{algorithm} 
      \caption{Dataset selection} \label{alg:data_sel}
      \footnotesize
      \begin{algorithmic}[1]
        \REQUIRE $\mathcal{D}$, the set of all datasets in AnalysisOps. Assume function size() exists for both dataset and set of datasets and returns total size in GB of dataset(s). Algorithm returns $\mathcal{S}$, the set of datasets to subscribe, and $\mathcal{R}$, the datasets to remove.
        \STATE $\mathcal{F} = \varnothing$
        \FORALL{$d \in \mathcal{D}$} \label{for1}
          \STATE{$\delta_f = 0$
            \FOR{$1$ \TO $w$} \label{for2}
              \STATE{Get $\delta_f$ for the day, multiply by exponential weight and add to total $\delta_f$}
            \ENDFOR
            \STATE Add touple $(d, \delta_f)$ to {\cal F}
          }
        \ENDFOR
        \STATE $\mathcal{S} = \varnothing$
        \STATE $\mathcal{R} = \varnothing$
        \LOOP \label{loop1}
          \STATE{Pop dataset $s$ with highest $\delta_f$ such that $\delta_f > 0$ \AND $\delta_{f+} > \overline{\delta_f}$.
            \IF{\NOT s} \label{if1}
              \STATE{Break loop}
            \ENDIF
            \IF{$\mathcal{S}$.size() + $s$.size() $>$ budget} \label{if2}
              \STATE{Break loop}
            \ENDIF
            \STATE $\mathcal{S} = \mathcal{S} \cup \{s\}$
            \WHILE{$(\mathcal{R}\mathrm{.size() + available\_size)} < \mathcal{S}\mathrm{.size()}$} \label{while1}
              \STATE{Pop dataset $r$ with lowest $\delta_f$ such that $\delta_f < 0$ \AND $\delta_{f-} < \overline{\delta_f}$.
                \IF{\NOT r} \label{if3}
                  \STATE{Break loop}
                \ENDIF
                \STATE $\mathcal{R} = \mathcal{R} \cup \{r\}$
              }
            \ENDWHILE
            \IF{$(\mathcal{R}\mathrm{.size()} + available_space) < \mathcal{S}\mathrm{.size()}$} \label{if4}
              \STATE{$\mathcal{S} = \mathcal{S} \setminus \{s\}$; Break loop} 
            \ENDIF
          }
        \ENDLOOP
        \RETURN $\mathcal{S}, \mathcal{R}$
      \end{algorithmic}
    \end{algorithm}

  \subsection{Site Selection}
    Site selection is decided based on average available CPU. Since the dataset replicas deleted should have very low current usage we can use the state before current changes to make decisions. Available CPU is calculated using equation \ref{eq:avail_cpu} where $\alpha$ is the sum of all CPU hours from the top three days during the last three weeks and $\beta$ is the number of CPU hours used yesterday.

    \begin{equation}\label{eq:avail_cpu}
      available\_cpu = \frac{\alpha}{3} - \beta
    \end{equation}

    Sites with the most avaialable CPU hours will be selected for new replicas. Of datasets selected for deletion the replica with the least number of accesses will be chosen for deletion. More restrictions is needed to avoid oversubscription to one site, using simulation to better understand effects of the currently proposed algorithm will be of great importance and is described in the following section.


\section{\textsc{Simulation and Testing}}
  \subsection{Simulation}
    It is important to understand the effects of these algorithms and find thresholds for when to subscribe and delete datasets before deploying them in a real system. A simulation testbed needs to be deployed to observe behaviours such as disk churn, network traffic, and performance. Evaluating results from a testbed will show flaws and strengths in the different parts of the dynamic data manager.

  \subsection{Testing}
    This section describes measurements used to evaluate the dynamic data manager.
    \begin{itemize}
      \item Number of unused datasets. A balanced system should decrease the number of unused replicas, if a replica is not used it should be deleted. If this number doesn't go down then there is an issue with the dynamic data manager.

      \item Minimize the median waiting time if there are sufficient available CPU's anywhere in the system. We want to place data in such a manner that the system is utilized as much as possible at all times. We are also interested in the median and not average as users will always be able to do weird things which affect the system but cannot be accounted for, such as submitting 10000 jobs to an unpopular dataset which only exists at a site which is currently fully utilized. A case which would be almost impossible to catch before it happens. The measurement of this test is expressed in equation \ref{eq:wait}.

      \begin{equation}\label{eq:wait}
        Q_t = \frac{\sum\limits_{d \in {\cal D}} \frac{\sum\limits_{j_d \in {\cal J}_d} qt}{|{\cal J}_d|}}{|{\cal D}|}
      \end{equation}

      Where ${\cal D}$ is the set of all datasets, ${\cal J}_d$ is the set of all jobs for dataset $d$, and $qt$ is the queue time of job $j_d$ while there was sufficient amount of CPU's available in the system. Where an optimal value for $Q_t$ would be $0$. Such a value would be virtually impossible to achieve, an acceptable value will have to be defined.
    \end{itemize}


\section{\textsc{Scheduler Integration}}
  To fully utilize a dynamic data management system it is important to have a tight coupling with the job schedulers. This will both provide instant popularity information about datasets for the agent and make it possible for the scheduler to quickly react to changes in data allocation. This functionality will become available with CRAB3.

\bibliographystyle{plain}
\bibliography{project_proposal.bib}

\end{document}
